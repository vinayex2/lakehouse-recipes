{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c94cfe8e-f301-4df1-a5b7-1e349a6400b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### SCD2 using Apply Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcf350c6-179d-4197-a6de-74257e5b4ede",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "project_name = 'scd2_dlt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0af02fc8-e14a-45f3-9e0d-dc7f5b8f7e14",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, expr\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType,DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eb21bb8-b033-4700-915a-8307c49e1e16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema_csv = StructType([\n",
    "    StructField('EmPId', IntegerType(),nullable=False),\n",
    "    StructField('FirstName', StringType(),nullable=False),\n",
    "    StructField('LastName', StringType(),nullable=False),\n",
    "    StructField('CreatedOn', DateType(),nullable=False),\n",
    "    StructField('ModifiedOn', DateType(),nullable=False)\n",
    "])\n",
    "\n",
    "cloud_file_options = {\n",
    "    'cloudFiles.format': 'csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88e0b9f4-af9f-430a-b32d-0bdc2fa4f70e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path = f'/FileStore/tables/{project_name}/data'\n",
    "dbutils.fs.mkdirs(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9bef1cc-bcb9-46da-a052-97a2add829d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name='bronze_employee'\n",
    ")\n",
    "def bronze_load():\n",
    "    df = spark.readStream.format('cloudFiles').options(**cloud_file_options).schema(schema_csv).load(project_path)\n",
    "    df = df.withColumn('file_processed_date', F.date_format(F.current_timestamp(),'yyyy-MM--dd HH:mm:ss'))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d98e0af1-2342-4012-9523-58f69cf1fb21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dlt.create_streaming_table(\n",
    "    name = 'silver_employee'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f9e63bf-43da-440d-aed4-c7704484aad6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dlt.apply_changes(\n",
    "    target='silver_employee',\n",
    "    source='bronze_employee',\n",
    "    keys=['EmPId'],\n",
    "    stored_as_scd_type='1',\n",
    "    sequence_by='file_processed_date'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ef73db3-e265-432a-adca-f7478199b5f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0;31mSignature:\u001B[0m \u001B[0mdlt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply_changes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtarget\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msource\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkeys\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msequence_by\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwhere\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mignore_null_updates\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mapply_as_deletes\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mapply_as_truncates\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolumn_list\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexcept_column_list\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstored_as_scd_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'1'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrack_history_column_list\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrack_history_except_column_list\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mflow_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0monce\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mignore_null_updates_column_list\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mignore_null_updates_except_column_list\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolumns_to_update\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;31mDocstring:\u001B[0m\nApply changes into the target table from the Change Data Capture (CDC) source. Target table must\nhave already been created using create_target_table function. Only one of column_list and\nexcept_column_list can be specified. Also only one of track_history_column_list and\ntrack_history_except_column_list can be specified.\n\nExample:\napply_changes(\n  target = \"target\",\n  source = \"source\",\n  keys = [\"key\"],\n  sequence_by = \"sequence_expr\",\n  ignore_null_updates = True,\n  column_list = [\"key\", \"value\"],\n  stored_as_scd_type = \"1\",\n  track_history_column_list = [\"value\"]\n)\n\nNote that for keys, sequence_by, column_list, except_column_list, track_history_column_list,\ntrack_history_except_column_list the arguments have to be column identifiers without qualifiers,\ne.g. they cannot be col(\"sourceTable.keyId\").\n\n:param target: The name of the target table that receives the Apply Changes.\n:param source: The name of the CDC source to stream from.\n:param keys: The column or combination of columns that uniquely identify a row in the source         data. This is used to identify which CDC events apply to specific records in the target         table. These keys also identify records in the target table, e.g., if there exists a record         for given keys and the CDC source has an UPSERT operation for the same keys, we will update         the existing record. At least one key must be provided. This should be a list of column         identifiers without qualifiers, expressed as either Python strings or Pyspark Columns.\n:param sequence_by: An expression that we use to order the source data. This can be expressed         as either a Python string or Pyspark Expression.\n:param where: An optional condition applied to both source and target during the execution         process to trigger optimizations such as partition pruning. This condition cannot be used to         drop source rows; that is, all CDC rows in the source must satisfy this condition, or an         error is thrown.\n:param ignore_null_updates: Whether to ignore the null value in the source data. For example,         consider the case where we have an UPSERT in the source data with null value for a column,         and this same column has non-null value in the target. If ignore_null_updates is true,         merging this UPSERT will not override the existing value for this column; If false,         merging will override the value for this column to null.\n:param apply_as_deletes: Delete condition for the merged operation. This should be a string of         expression e.g. \"operation = 'DELETE'\"\n:param apply_as_truncates: Truncate condition for the merged operation. This should be a string         expression e.g. \"operation = 'TRUNCATE'\"\n:param column_list: Columns that will be included in the output table. This should be a list         of column identifiers without qualifiers, expressed as either Python strings or Pyspark         Column. Only one of column_list and except_column_list can be specified.\n:param except_column_list: Columns that will be excluded in the output table. This should be a         list of column identifiers without qualifiers, expressed as either Python strings or Pyspark         Column. Only one of column_list and except_column_list can be specified. When this is         specified, all columns in the dataframe of the target table except those in this list will         be in the output table.\n:param stored_as_scd_type: Specify the SCD Type for output format. 1 for SCD Type 1 and 2 for         SCD Type 2. This parameter can either be an integer or string. Default value is 1.\n:param track_history_column_list: Columns that will be tracked for change history.         This should be a list of column identifiers without qualifiers, expressed as either Python         strings or Pyspark Column. Only one of track_history_column_list and         track_history_except_column_list can be specified.\n:param track_history_except_column_list: Columns that will not be tracked for change history.         Those columns will reflect the values that were seen before the next update to any of the         tracked columns.         This should be a list of column identifiers without qualifiers, expressed as either Python         strings or Pyspark Column. Only one of track_history_column_list and         track_history_except_column_list can be specified.\n:param flow_name: The name of the flow for this apply_changes command. When unspecified this will build a            \"default flow\" with name equal to the target name.\n:param ignore_null_updates_column_list: subset of columns to ignore null in updates.\n:param ignore_null_updates_except_column_list: subset of columns excluded from ignoring null in updates.\n:param columns_to_update: Column indicating which user columns to update or ignore.\n\u001B[0;31mFile:\u001B[0m      /databricks/spark/python/dlt/api.py\n\u001B[0;31mLine:\u001B[0m      877\n\u001B[0;31mType:\u001B[0m      function"
     ]
    }
   ],
   "source": [
    "?dlt.apply_changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c804d31-b9d9-4536-bcf7-761d806e8cf5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### SCD1 Trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e3ca3f8-332e-4b6d-86d1-ef2c423b05a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dlt.apply_changes(\n",
    "    target='silver_employee',\n",
    "    source='bronze_employee',\n",
    "    keys=['EmPId'],\n",
    "    stored_as_scd_type='1',\n",
    "    sequence_by='file_processed_date',    \n",
    "    flow_name=f'{project_name}_bronze2silver'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83ad9620-c1d0-4bd4-9e07-93584ac0b989",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### SCD2 Trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2725cb76-0a06-4908-bd59-3aabe0c0d60a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dlt.apply_changes(\n",
    "    target='silver_employee',\n",
    "    source='bronze_employee',\n",
    "    keys=['EmPId'],\n",
    "    stored_as_scd_type='2',\n",
    "    sequence_by='file_processed_date',\n",
    "    track_history_column_list=['file_processed_date'],\n",
    "    flow_name=f'{project_name}_bronze2silver'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD1_SCD2_with_DLT_python",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
